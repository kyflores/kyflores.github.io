<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-10-21T15:48:52-10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kyle’s code attic</title><subtitle>Welcome to my page. I plan to use this site to document some of my projects around embedded systems, edge AI, computer vision, and a mix of other things.
Frequently when scouring the internet for solutions to obscure problems, I find the real answer on someone&apos;s personal dev blog. I hope others can find the same value here.</subtitle><entry><title type="html">Preparing a custom model with Vitis AI</title><link href="http://localhost:4000/xilinx/edge_ai/quantization/vitis/2023/10/01/quantizing.html" rel="alternate" type="text/html" title="Preparing a custom model with Vitis AI" /><published>2023-10-01T12:27:00-10:00</published><updated>2023-10-01T12:27:00-10:00</updated><id>http://localhost:4000/xilinx/edge_ai/quantization/vitis/2023/10/01/quantizing</id><content type="html" xml:base="http://localhost:4000/xilinx/edge_ai/quantization/vitis/2023/10/01/quantizing.html">&lt;h1 id=&quot;preparing-a-custom-model-with-vitis-ai&quot;&gt;Preparing a custom model with Vitis AI&lt;/h1&gt;
&lt;p&gt;The whole point of getting the Vitis AI container downloaded in the last post
was to generate our own model files for the Xilinx DPU. Xilinx has many &lt;a href=&quot;https://github.com/Xilinx/Vitis-AI/tree/v2.5/model_zoo&quot;&gt;premade models available&lt;/a&gt;
but you’ll eventually need to create your own if nothing up there matches your needs.&lt;/p&gt;

&lt;h2 id=&quot;fashionmnist&quot;&gt;FashionMNIST&lt;/h2&gt;
&lt;p&gt;The model I’m going to be quantizing is a classifier with 2 convs and 2 linear layers trained
on FashionMNIST, a convenient dataset that’s readily available through torchvision. To be clear,
it’s a minimal example to show the process, and small enough to be trained + quantized on a CPU
in a few minutes. You can find the model training code &lt;a href=&quot;https://github.com/kyflores/kv260-vitis-flow/blob/main/fmnist_train.py&quot;&gt;here&lt;/a&gt; in one of my repos.&lt;/p&gt;

&lt;p&gt;After training you should see ~90% accuracy, and end up with at least a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fmnist.pt&lt;/code&gt; set of weights.
This is about where you’ll be starting if you’re following along with a custom model. Up to this point,
there shouldn’t be any requirement to use the special Vitis AI docker containers or even match their
PyTorch version.&lt;/p&gt;

&lt;h2 id=&quot;quantization&quot;&gt;Quantization&lt;/h2&gt;
&lt;p&gt;To create a model for the Xilinx DPU, we need a quantized set of weights. In short, quantization
is the process of converting a model whose weights are in 32bit float precision down to one
with only 8bit integer precision. That’s &lt;em&gt;a lot&lt;/em&gt; less precision, but also a much smaller range.
After all, int8’s can only be -128 to 127! My shallow understanding of the quantization process
is that it will need to find some additional offsets and scale factors for regions of the
network that allow it to be evaluated without saturating the int8’s, but to do so it needs to
see some representative examples of what might come in at test time.&lt;/p&gt;

&lt;p&gt;Now we’re going to need those Xilinx docker images.
On your host/workstation…&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Get Vitis-AI v2.5, which I&apos;ll be just calling vitis
docker pull xilinx/vitis-ai-cpu:2.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;On the Kria…&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Get Kria runtime based on Vitis runtime/library v2.5, which I&apos;ll call kria-runtime
docker pull xilinx/kria-runtime:2022.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;A note here about versions; I began with the Vitis v2.5 host containers, but the outputs from
newer versions like v3.5 appear to be compatible with kria-runtime. If you want CUDA support,
you need to &lt;a href=&quot;https://github.com/Xilinx/Vitis-AI/tree/master/docker&quot;&gt;build it yourself&lt;/a&gt; though.
I also tried to rebuild v2.5’s container with GPU support, but couldn’t get the old image to build…
seems like some packages (and even the base image) are no longer available, which is surprising
since that image is not even 2 years old.&lt;/p&gt;

&lt;h3 id=&quot;inspecting&quot;&gt;Inspecting&lt;/h3&gt;
&lt;p&gt;I’ll be referring to &lt;a href=&quot;https://github.com/kyflores/kv260-vitis-flow&quot;&gt;my repo&lt;/a&gt; again here, specifically &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fminst_quantize_pt1.py&lt;/code&gt; right now.&lt;/p&gt;

&lt;p&gt;The first step, inspecting the model, lets us know if the torch model we want to quantize can
be mapped to the DPU’s supported operations, and what outstanding operators will end up mapped
to the CPU runner instead. We need two things to inspect: a dummy tensor matching the shape of
our input data, and the name of the DPU we’re going to be compiling for. The dummy input is easy,
but to get the DPU name, you can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xdputil query&lt;/code&gt; on the Kria (in the container) after loading
the bitstream/app with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmutil load &amp;lt;appname&amp;gt;&lt;/code&gt; (outside of the container).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;DPUCZDX8G_ISA1_B3136&quot;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (or 0x101000016010406)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inspector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Inspector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dummy_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batchsize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inspector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inspect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dummy_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inspect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Along with a ton of text output, you’ll get a visual graph of the model’s operators. My simple
classifier has this graph for example.
&lt;img src=&quot;/assets/inspect_DPUCZDX8G_ISA1_B3136.png&quot; alt=&quot;Classifier compute graph&quot; width=&quot;500&quot; /&gt;
If every node says &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assigned device:dpu&lt;/code&gt;, you’re good to go, but if not you’ll have to go back
and reconfigure your model to adhere to the &lt;a href=&quot;https://docs.xilinx.com/r/en-US/pg338-dpu/Introduction?tocId=4lq1FtJ078vxzAJQVMkl_g&quot;&gt;supported operators&lt;/a&gt;.
If the github issues are anything to go by, a common problem was support for different activation functions.
The operators table shows that only ReLU, ReLU6, LeakyReLU, Hard Sigmoid, and Hard Swish are allowed,
which notably omits the SiLU activation favored by the newer YOLO variants, or GeLU as popularized by BERT.
I’m not sure if it can be changed in place, but from my relatively shallow knowledge of deep learning,
I’d expect that the network would have to be retrained, or at least finetuned to accomodate a change
in activations.&lt;/p&gt;

&lt;h3 id=&quot;calbration&quot;&gt;Calbration&lt;/h3&gt;
&lt;p&gt;Once you’ve got something that you know will map nicely to the DPU, you can finetune the network
using Xilinx’s quantizing API. Specifically, this thing:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch_quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;calib&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dummy_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;quant_config_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fast_finetune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quant_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quant_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When constructing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch_quantizer&lt;/code&gt;, we tell it what mode to operate in (‘calib’), give it our
&lt;em&gt;PyTorch model object&lt;/em&gt;, the same dummy input as passed to the inspector, and the compute device to run
on. It’s a bit odd, but there are several other steps to perform after that use this same function with
a different operation mode parameter. Personally I think it would have been clearer to just have separate
classes, but I digress.&lt;/p&gt;

&lt;p&gt;Going a bit out of order, in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fast_finetune&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;val_loader&lt;/code&gt; is a dataloader for handling your calibration
dataset. The calibration set can be significantly smaller than either your training or evaluation sets,
and IIRC the recommendation was in the range of 100-1000 images, as long as they’re representative of
what the model will see in practice.&lt;/p&gt;

&lt;p&gt;I’m a bit unclear on what’s strictly required for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fast_finetune&lt;/code&gt; to work though; it cannot be called
with no arguments but &lt;a href=&quot;https://docs.xilinx.com/r/1.3-English/ug1414-vitis-ai/Module-Partial-Quantization&quot;&gt;doc pages&lt;/a&gt; and &lt;a href=&quot;https://github.com/Xilinx/Vitis-AI/issues/787&quot;&gt;replies by Xilinx engineers&lt;/a&gt;, also say we don’t need labels or a loss function, just a way to call forward().
Anyway, this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;evaluate&lt;/code&gt; function needs to run a forward pass of the model over your input data, and the
tuple passed in under it should match its arguments. I modeled mine after &lt;a href=&quot;https://github.com/Xilinx/Vitis-AI/blob/master/src/vai_quantizer/vai_q_pytorch/example/resnet18_quant.py&quot;&gt;Xilinx’s resnet example&lt;/a&gt;,
but their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;evaluate&lt;/code&gt; is reused in several ways. Also, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fast_finetune&lt;/code&gt; is optional; running it can improve
performance, but it can be skipped entirely if you’ve found it makes no difference.&lt;/p&gt;

&lt;p&gt;Finally, we just need to evaluate the model with our sample calibration dataset, and an easy way to do this
is recycle a dataloader from your training code. I was stuck at this point for awhile; I had forgotten to
evaluate the model at the end, thinking the evaluations &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fast_finetune&lt;/code&gt; were enough. All the expected
output files existed, but were “empty” or filled with default values, which was confusing.&lt;/p&gt;

&lt;h3 id=&quot;exporting&quot;&gt;Exporting&lt;/h3&gt;
&lt;p&gt;After quantizing + optional finetuning, the next step is to export the model. Admittedly, I don’t
have much of an idea of what happens in this step, or why it has certain requirements, but all we
need to do is build another &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch_quantizer&lt;/code&gt; with the first arg (mode) set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test&lt;/code&gt;, then
evaluate it once with any data of the correct shape with a batch size of 1.&lt;/p&gt;

&lt;p&gt;I got confused here for awhile because it seems to work without explicitly passing in the quantized
to the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch_quantizer&lt;/code&gt; instance, but AFAICT it assumes the outputs from the last step are in
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_result/&lt;/code&gt; directory, although there must be some options to change this.&lt;/p&gt;

&lt;h2 id=&quot;compiling&quot;&gt;Compiling&lt;/h2&gt;
&lt;p&gt;Once we’ve got our *.xmodel from the export step, the last thing to do is compile for a specific
device architecture, in this case the Kria’s DPU B3136.&lt;/p&gt;

&lt;p&gt;First go and grab the DPU fingerprint from the Kria board if you don’t have it already.
You can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xdputil query&lt;/code&gt; &lt;em&gt;INSIDE&lt;/em&gt; the kria-runtime container after loading any of the Xilinx apps
like smartvision with a DPU instantiated &lt;em&gt;OUTSIDE&lt;/em&gt; the container with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmutil loadapp kv260-nlp-smartvision&lt;/code&gt;
For some reason I had to install numpy for xputil to work, not sure why it wasn’t in the base image.
In the output you should see this line, or something like it if you’re not on the Kria.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;fingerprint&quot;:&quot;0x101000016010406&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Make a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arch.json&lt;/code&gt; with that as the only entry; we’ll need it in just a sec.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arch.json&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;fingerprint&quot;:&quot;0x101000016010406&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Back in the Vitis host container, we just need to invoke the compiler with our model and DPU arch.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vai_c_xir -x quantize_result/MyCnn_int.xmodel -a arch.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;After that you should have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deploy.xmodel&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;md5sum.txt&lt;/code&gt; in your working directory if everything
goes smoothly. If you aren’t using PyTorch, you should invoke a different vai_c such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vai_c_tensorflow2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The last thing we need to do is make sure those weights work, so go ahead and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; then back to the kria,
and start an instance of kria-runtime (again) to use xdputil.
Finally &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xdputil xmodel deploy.xmodel -l&lt;/code&gt; should show one subgraph. We can then benchmark it.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@xlnx-docker:/run/host/Documents/quantize_result# xdputil benchmark deploy.xmodel 2
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1003 23:15:44.922972   665 test_dpu_runner_mt.cpp:474] shuffle results for batch...
I1003 23:15:44.923450   665 performance_test.hpp:73] 0% ...
I1003 23:15:50.923636   665 performance_test.hpp:76] 10% ...
I1003 23:15:56.923797   665 performance_test.hpp:76] 20% ...
I1003 23:16:02.923959   665 performance_test.hpp:76] 30% ...
I1003 23:16:08.924115   665 performance_test.hpp:76] 40% ...
I1003 23:16:14.924278   665 performance_test.hpp:76] 50% ...
I1003 23:16:20.924437   665 performance_test.hpp:76] 60% ...
I1003 23:16:26.924599   665 performance_test.hpp:76] 70% ...
I1003 23:16:32.924762   665 performance_test.hpp:76] 80% ...
I1003 23:16:38.924928   665 performance_test.hpp:76] 90% ...
I1003 23:16:44.925087   665 performance_test.hpp:76] 100% ...
I1003 23:16:44.925154   665 performance_test.hpp:79] stop and waiting for all threads terminated....
I1003 23:16:44.925801   665 performance_test.hpp:85] thread-0 processes 199424 frames
I1003 23:16:44.925851   665 performance_test.hpp:85] thread-1 processes 199455 frames
I1003 23:16:44.925873   665 performance_test.hpp:93] it takes 697 us for shutdown
I1003 23:16:44.925894   665 performance_test.hpp:94] FPS= 6647.71 number_of_frames= 398879 time= 60.0024 seconds.
I1003 23:16:44.925951   665 performance_test.hpp:96] BYEBYE
Test PASS.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All that work and we’re &lt;em&gt;still&lt;/em&gt; not done. Next time we’ll add a cpp utility to run inference on an image.&lt;/p&gt;

&lt;h2 id=&quot;model-inspector&quot;&gt;Model inspector&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DPU Arch&quot;:&quot;DPUCZDX8G_ISA1_B3136&quot;
&quot;fingerprint&quot;:&quot;0x101000016010406&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="xilinx" /><category term="edge_ai" /><category term="quantization" /><category term="vitis" /><summary type="html">Preparing a custom model with Vitis AI The whole point of getting the Vitis AI container downloaded in the last post was to generate our own model files for the Xilinx DPU. Xilinx has many premade models available but you’ll eventually need to create your own if nothing up there matches your needs.</summary></entry><entry><title type="html">Custom model inference on VART</title><link href="http://localhost:4000/xilinx/edge_ai/inference/vitis/2023/10/01/inference.html" rel="alternate" type="text/html" title="Custom model inference on VART" /><published>2023-10-01T12:27:00-10:00</published><updated>2023-10-01T12:27:00-10:00</updated><id>http://localhost:4000/xilinx/edge_ai/inference/vitis/2023/10/01/inference</id><content type="html" xml:base="http://localhost:4000/xilinx/edge_ai/inference/vitis/2023/10/01/inference.html">&lt;h2 id=&quot;model-inference&quot;&gt;Model inference&lt;/h2&gt;
&lt;p&gt;In the last post, we managed to quantize a PyTorch model with Vitis-AI, and export its int8
weights and graph for use with Vitis AI Runtime (VART). Now, we still need some code to drive
VART, so let’s look at that now.&lt;/p&gt;

&lt;h3 id=&quot;a-note-about-vitis-ai-levels&quot;&gt;A note about Vitis AI levels&lt;/h3&gt;
&lt;p&gt;VART has several levels of abstraction they call API_x that make different assumptions
about your model. The levels 1 and 2 appear to work only with models matching one of
the architectures in their model zoo, while level 0 is more work to use but can work with just abot
anything, and level 3 is required for models with custom ops or multiple subgraphs.
Check out the &lt;a href=&quot;https://docs.xilinx.com/r/en-US/ug1354-xilinx-ai-sdk/Programming-Examples&quot;&gt;full docs here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The API_0 sample is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Vitis-AI/examples/vai_runtime/resnet50/src/main.cc&lt;/code&gt;, so we’ll be
basing our code off of that and some &lt;a href=&quot;https://xilinx.github.io/Vitis-AI/3.5/html/doxygen/api/class/classvart_1_1_runner.html&quot;&gt;VART documentation&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;code-sample&quot;&gt;Code sample&lt;/h3&gt;
&lt;p&gt;I’ll be referring to &lt;a href=&quot;https://github.com/kyflores/kv260-vitis-flow/blob/main/cpp/main.cc&quot;&gt;this file&lt;/a&gt;.
Apologies in advance for some janky c++…Maybe if you’re reading this in the future I’ll
have cleaned this up a bit, but some of the VART API stuff threw me for a loop.
my goal here was to get everything that matters into one file, which almost succeeded, except
for a dependency on &lt;a href=&quot;https://github.com/nothings/stb&quot;&gt;stb_image&lt;/a&gt; for reading images. If you don’t
know about &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stb&lt;/code&gt;, it’s definitely worth a look for single-file, header only code for common tasks.&lt;/p&gt;

&lt;p&gt;This all starts off by reading an image (which I separately pulled from the FashionMNIST dataset),
and the xmodel file we produced last time. There’s several subgraphs in the xmodel file, but only
one (for this network) will have the device attribute set to DPU.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;constexpr&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MODELNAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;quantize_result/deploy.xmodel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor_from_stbimage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deserialize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODELNAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph_root&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_root_subgraph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;children&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph_root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;children_topological_sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Check subgraphs on CLI with `xdputil xmodel quantize_result/deploy.xmodel -l`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Subgraph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subgraph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;children&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;device&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Device: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;DPU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;subgraph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runner&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_runner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subgraph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;run&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this section we get a handle of sorts to the input and output tensors from the graph
This API confused me a bit; I’m used to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor&lt;/code&gt; storing both the metadata like shape as well
as the data like in Torch, but this VART tensor class seems to be only a description of what
sort of data is required at the inputs and outputs. There’s a separate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TensorBuffer&lt;/code&gt; class
in VART that holds the data, and we’ll see that a few lines down.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_scale&lt;/code&gt; is a helper function up above that gets the value of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fix_point&lt;/code&gt; attribute in
the input and output tensors and converts it to a scale factor. This fix_point value tells us
how to convert a floating point value to an int8 in the range this model needs at the inputs and
outputs. Keep in mind for a moment too that at the input, we’re converting float-&amp;gt;int8, and at
the output we’re doing int8-&amp;gt;float, so at the output we need to divide by the scale factor instead.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_tensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_input_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_scale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_tensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_output_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_scale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Input scale &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_scale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Output scale &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_scale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next step is to create TensorBuffers that will hold the actual inputs and outputs.
We’re using this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vart::alloc_cpu_flat_tensor_buffer(input)&lt;/code&gt; helper function to do this
for us. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TensorBuffer&lt;/code&gt; is actually a base class in VART, and I assume the classes derived from
it hold buffers in different locations, whether that’s main RAM, or an HBM stack private
to the FPGA. Since the Kria’s FPGA doesn’t have its own dedicated memory, we’re sharing the
CPU’s 4GB memory here, thus CpuFlatTensorBuffer. This TensorBuffer has a bit of an awkward
API though, not sure why &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CpuFlatTensorBuffer::data()&lt;/code&gt; needs to return its data pointer as a
u64 that we need to reinterpret cast back to actually use. Xilinx holds it as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;void*&lt;/code&gt; inside
the class and reinterpret_casts it before returning it.&lt;/p&gt;

&lt;p&gt;One other note here is that the training and quantiziation code scaled all the image input
values to be between 0 and 1. The inference code needs to do the same so that the calibrated
ranges are valid.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alloc_cpu_flat_tensor_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// Copy data by ptr into the buffer&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// This CpuFlatTensorBuffer class returns a uint64_t by reinterpret casting its 64bit data&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// pointer into an integer, wtf? Need to undo that here to get something we can use as a ptr&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_ptr_int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;reinterpret_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_bytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Input tensor is &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_bytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; bytes long&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_bytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// In our training code, the uint8 images are squished to the range (0, 1)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// before being input to the network, so replicate that here.&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;raw_ptr_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;input_tensor_buffers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emplace_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Before launching the DPU, we need to call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sync_for_write&lt;/code&gt; on our input, then gather
the TensorBuffer pointers together for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;execute_async&lt;/code&gt;. This looks kind of hacky to me,
so I might be using the API wrong here…but up above &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alloc_cpu_flat_tensor_buffer&lt;/code&gt; returns
its buffers behind a unique_ptr, but execute_async requires a raw pointer.&lt;/p&gt;

&lt;p&gt;Then we wait for the computation to finish and finally sync the output buffer.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_tensor_buffers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sync_for_write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_data_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Executing runner...&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ptrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_tensor_buffers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ptrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_ptrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_tensor_buffers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_ptrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute_async&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ptrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_ptrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_tensor_buffers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sync_for_read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_data_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We’re basically done at this point. To wrap up we just run softmax over the outputs.
Unfortunately there’s no built softmax operator on the DPU, and I didn’t see one in
VART either, so here’s a naive softmax implementation to use instead. Note that this
won’t work very well if your inputs get too large…&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;denom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;denom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;denom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_buf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_tensor_buffers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_buf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_ptr_int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;reinterpret_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_bytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Output tensor is &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_bytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; bytes long&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_ptr_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_bytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At long last…our results! This image was from label index 1 (from 0 to 9),
and we can see at the bottom that we predicted 99.63% confidence for label 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/fmnist_1.jpg&quot; alt=&quot;sample&quot; width=&quot;250&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x, y, n: 28 28 1
Device: USER
subgraph_MyCnn__input_0
Device: DPU
subgraph_MyCnn__MyCnn_Conv2d_conv1_1__ret_3
Device: CPU
subgraph_MyCnn__MyCnn_Linear_lin2__ret_fix_
Input scale 64
Output scale 8
Populating input tensor
1 Input(s)
Input tensor is 784 bytes long
1 Output(s)
Executing runner...
Output tensor is 10 bytes long

3.52274e-05 0.996319 2.55186e-06 0.00359329 4.52328e-05 9.20345e-09 3.71294e-06 2.20779e-08 6.45211e-07 1.63135e-07
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="xilinx" /><category term="edge_ai" /><category term="inference" /><category term="vitis" /><summary type="html">Model inference In the last post, we managed to quantize a PyTorch model with Vitis-AI, and export its int8 weights and graph for use with Vitis AI Runtime (VART). Now, we still need some code to drive VART, so let’s look at that now.</summary></entry><entry><title type="html">Setting up the Xilinx KV260</title><link href="http://localhost:4000/xilinx/kv260/edge_ai/2023/09/28/kv260.html" rel="alternate" type="text/html" title="Setting up the Xilinx KV260" /><published>2023-09-28T17:37:00-10:00</published><updated>2023-09-28T17:37:00-10:00</updated><id>http://localhost:4000/xilinx/kv260/edge_ai/2023/09/28/kv260</id><content type="html" xml:base="http://localhost:4000/xilinx/kv260/edge_ai/2023/09/28/kv260.html">&lt;h1 id=&quot;xilinx-kv260&quot;&gt;Xilinx KV260&lt;/h1&gt;
&lt;p&gt;The KV260 is a relatively recent (2021) Xilinx product based on the Zynq Ultrascale
family, chips with Cortex-A53 processors and a variable amount of logic elements.
&lt;img src=&quot;/assets/kv260.jpg&quot; alt=&quot;KV260&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That being said, the Kria SOMs and their ecosystem are a bit unique compared to Xilinx’s
historical offerings for a few reasons.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Premade Ubuntu LTS rootfs and Xilinx PPAs for additional software&lt;/li&gt;
  &lt;li&gt;Premade FPGA firmware images with the DPU (that’s their ML accelerator IP) embedded&lt;/li&gt;
  &lt;li&gt;Basic GPU and video codecs present&lt;/li&gt;
  &lt;li&gt;Unusually low price of ~$250 for the developer kit, for a similar feature set to
the more expensive Ultrascale+ EV&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although it can be used like any other Xilinx product with Vivado, Petalinux, etc, and
probably could serve as a great platform for getting a decently sized FPGA cheaply, the
Kria modules definitely feel more geared towards software developers trying to deploy
ML vision models, or anyone more familiar with single board computers like the Pi.
I’m not sure if it really competes with the Tegra family in practice since its compute
throughput is much lower but it seems aimed at the same use case.&lt;/p&gt;

&lt;p&gt;I bought a KV260 devkit when it released back in 2021 while thinking of all the interesting
projects I could use it for, but as things often do, it went unused.
Now that the Ryzen 7040 mobile chips with XDNA AI accelerators have come out, and AMD
has &lt;a href=&quot;https://github.com/amd/RyzenAI-cloud-to-client-demo&quot;&gt;published some sample code&lt;/a&gt;, I’ve decided to dust off the Kria and try to make use of Vitis AI in anticipation of a Linux
port of the Ryzen AI SDK (plz AMD).&lt;/p&gt;

&lt;p&gt;Anyway, to get started, pull up the &lt;a href=&quot;https://xilinx.github.io/kria-apps-docs/kv260/2022.1/build/html/index.html&quot;&gt;Kria docs&lt;/a&gt; in a tab, and read on.
The Xilinx atlassian wiki shows up a lot in search results but looks out of sync on some points,
so the linked site seems to be most up to date.&lt;/p&gt;

&lt;h2 id=&quot;running-the-nlp-smartcamera-application&quot;&gt;Running the NLP smartcamera application&lt;/h2&gt;
&lt;p&gt;Here are the condensed instructions to get up and running. The info here is all available in
their official docs, but it’s scattered across several pages on different sites.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Flash the &lt;a href=&quot;https://ubuntu.com/download/amd&quot;&gt;KV260 ubuntu 22.04 image&lt;/a&gt; onto an SD card&lt;/li&gt;
  &lt;li&gt;Login with User: ubuntu, Password: ubuntu. You’ll be asked to change it.&lt;/li&gt;
  &lt;li&gt;I had some minor issues with the Ubuntu image’s out-of-box configuration
    &lt;ul&gt;
      &lt;li&gt;The unattended upgrades service takes a long time to run to the point that it
can appear to be stuck.
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo systemctl stop unattended-upgrades.service; sudo apt-get purge unattended-upgrades&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;DNS didn’t start for some reason.
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;systemctl restart systemd-resolved&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The cloud init service waits for several minutes when booting
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo touch /etc/cloud/cloud-init.disabled&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo snap install xlnx-config --classic --channel=2.x&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo xlnx-config.sysinit&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://xilinx.github.io/kria-apps-docs/kr260/build/html/docs/kria_starterkit_linux_boot.html&quot;&gt;If you get an error flashing kernel, just try again apparently…&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt install xlnx-firmware-kv260-nlp-smartvision&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;AFAICT &lt;a href=&quot;https://github.com/Xilinx/kria-apps-firmware/tree/main/k26-dfx/2rp&quot;&gt;this is the repo that package is built from&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://xilinx.github.io/kria-apps-docs/kv260/2022.1/build/html/docs/nlp-smartvision/docs/app_deployment_nlp.html&quot;&gt;Now following the instructions for the nlp-smartcamera…&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo xmutil listapps&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo xmutil unloadapp&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo xmutil loadapp kv260-nlp-smartvision&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt install docker.io&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo usermod -aG docker ubuntu&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull xilinx/nlp-smartvision:2022.1&lt;/code&gt; This container is &amp;gt;1GB.&lt;/li&gt;
      &lt;li&gt;
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run \
    --env=&quot;DISPLAY&quot; \
    -h &quot;xlnx-docker&quot; \
    --env=&quot;XDG_SESSION_TYPE&quot; \
    --net=host \
    --privileged \
    -v /tmp:/tmp \
    -v /dev:/dev \
    -v /sys:/sys \
    -v /etc/vart.conf:/etc/vart.conf \
    -v /lib/firmware/xilinx:/lib/firmware/xilinx \
    -v /run:/run \
    -it xilinx/nlp-smartvision:2022.1 bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Finally, get a random jpg online with some everyday objects in it, copy it
to one of the paths mounted in the container like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp&lt;/code&gt;and try:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nlp-smartvision --test /tmp/pic.jpg yolov2_voc_pruned_0_77&lt;/code&gt; in the container!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The purpose of loading one of Xilinx’s demo apps is to get the DPU, their ML accelerator
into the SoC’s FPGA fabric. The DPU isn’t tied to a single model, so it should be possible
to deploy our own CNNs to it with some effort.&lt;/p&gt;

&lt;p&gt;The process of making a bitstream with the DPU inside is a whole nother problem that would
require installing the entire Vivado/Vitis tool suite, so it’s nice that we can leverage
some of the prebuilt resources.&lt;/p&gt;

&lt;h2 id=&quot;building-stuff-with-the-kria-docker-images&quot;&gt;Building stuff with the Kria docker images&lt;/h2&gt;
&lt;p&gt;I wasn’t expecting docker to be their approved way of running apps, but I think it works out.
An advantage of Xilinx making everything possible in a container is the app can be built offline
easily by just pulling the container on a host with qemu-user-static and binfmt setup to run
aarch64 binaries. NVIDIA pushes users towards a similar flow on the Tegra platform, so maybe that’s
just where the embedded AI stuff is going. That being said, containers are more of a convenience
here, the options needed to make it work throw out any semblance of isolation or security.&lt;/p&gt;

&lt;p&gt;The demo is nice and all, but anyone following this post probably cares about making
a custom application. The most “software developer” friendly way of doing this seems to be
using the Xilinx Kria docker images, so let’s start there and rebuild the nlp-smartvision app.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First, I’m going to install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qemu-user-static&lt;/code&gt; on my Ubuntu-based host machine. That
combined with binfmt lets you run binaries from other architectures on your machine.
Since the Kria modules have aarch64 CPUs, we need this to run the Kria containers.&lt;/li&gt;
  &lt;li&gt;Now if I do &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run -it --rm xilinx/kria-developer:latest&lt;/code&gt;, I should end up in the
container and be able to use basic utilities like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Inside the container, I’ll clone the app’s repo, https://github.com/Xilinx/nlp-smartvision,
and do the usual cmake incantation. Thankfully this built without errors!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;building-the-vitis-ai-container&quot;&gt;Building the Vitis AI container&lt;/h2&gt;
&lt;p&gt;I’ll add this step here too since it’s just more busywork, but next I have to build the
Vitis AI container. The tools for quantizing (more on that later) and a distribution of
PyTorch is in here, which we’ll need for producing our own model weights. There are
some prebuilt Vitis AI containers on dockerhub, but if you’ve got an NVIDIA card you’ll need
to build it yourself. Figures.&lt;/p&gt;

&lt;p&gt;Since the version of Vitis AI in the repos is v2.5, I’ll start by trying to build the container
from the v2.5 tag.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/Xilinx/Vitis-AI.git --branch v2.5
cd Vitis-AI/docker
bash docker_build_gpu.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And here’s the first blocker. The base image used is no longer on docker hub. I changed it to
a newer CUDA 11.8 version with a similar image, nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu18.04,
but this &lt;em&gt;also&lt;/em&gt; fails to build, hanging indefinitely when trying to install the PyTorch
environment’s packages. It looks conda can’t find some of the Xilinx conda packages, perhaps
they’re no longer hosted online or something, but at any rate I decided to pull the CPU-only
Vitis AI v2.5 container at this point, which &lt;em&gt;is&lt;/em&gt; on docker hub &lt;a href=&quot;https://hub.docker.com/r/xilinx/vitis-ai-cpu/tags&quot;&gt;here&lt;/a&gt;.
Simply doing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull xilinx/vitis-ai-cpu:2.5.0&lt;/code&gt; will get you that image.&lt;/p&gt;

&lt;p&gt;I would have liked if a newer version of Vitis AI was available for the Kria, but the Ubuntu PPA
that provides those packages is still on 2.5. I started making my own dockerfile to compile v3.5
of vitis-ai-runtime, vitis-ai-library, vvas-essentials, but it’s still WIP.&lt;/p&gt;

&lt;p&gt;Anyway, we’ll leave it there for now and continue in the next post.&lt;/p&gt;

&lt;h2 id=&quot;useful-links&quot;&gt;Useful links&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Kria Docker https://github.com/Xilinx/kria-docker&lt;/li&gt;
  &lt;li&gt;Vitis AI v2.5 Docker image https://hub.docker.com/r/xilinx/vitis-ai-cpu/tags&lt;/li&gt;
  &lt;li&gt;NLP smartvision app https://github.com/Xilinx/nlp-smartvision&lt;/li&gt;
  &lt;li&gt;Kria prebuilt firmware https://github.com/Xilinx/kria-apps-firmware/tree/main/k26-dfx/2rp&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="xilinx" /><category term="kv260" /><category term="edge_ai" /><summary type="html">Xilinx KV260 The KV260 is a relatively recent (2021) Xilinx product based on the Zynq Ultrascale family, chips with Cortex-A53 processors and a variable amount of logic elements.</summary></entry></feed>