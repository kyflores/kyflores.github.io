<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Custom model inference on VART | Kyle’s code attic</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Custom model inference on VART" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Model inference In the last post, we managed to quantize a PyTorch model with Vitis-AI, and export its int8 weights and graph for use with Vitis AI Runtime (VART). Now, we still need some code to drive VART, so let’s look at that now." />
<meta property="og:description" content="Model inference In the last post, we managed to quantize a PyTorch model with Vitis-AI, and export its int8 weights and graph for use with Vitis AI Runtime (VART). Now, we still need some code to drive VART, so let’s look at that now." />
<link rel="canonical" href="http://localhost:4000/xilinx/edge_ai/inference/vitis/2023/10/01/inference.html" />
<meta property="og:url" content="http://localhost:4000/xilinx/edge_ai/inference/vitis/2023/10/01/inference.html" />
<meta property="og:site_name" content="Kyle’s code attic" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-10-01T12:27:00-10:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Custom model inference on VART" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-10-01T12:27:00-10:00","datePublished":"2023-10-01T12:27:00-10:00","description":"Model inference In the last post, we managed to quantize a PyTorch model with Vitis-AI, and export its int8 weights and graph for use with Vitis AI Runtime (VART). Now, we still need some code to drive VART, so let’s look at that now.","headline":"Custom model inference on VART","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/xilinx/edge_ai/inference/vitis/2023/10/01/inference.html"},"url":"http://localhost:4000/xilinx/edge_ai/inference/vitis/2023/10/01/inference.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Kyle&apos;s code attic" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kyle&#39;s code attic</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Custom model inference on VART</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-10-01T12:27:00-10:00" itemprop="datePublished">Oct 1, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="model-inference">Model inference</h2>
<p>In the last post, we managed to quantize a PyTorch model with Vitis-AI, and export its int8
weights and graph for use with Vitis AI Runtime (VART). Now, we still need some code to drive
VART, so let’s look at that now.</p>

<h3 id="a-note-about-vitis-ai-levels">A note about Vitis AI levels</h3>
<p>VART has several levels of abstraction they call API_x that make different assumptions
about your model. The levels 1 and 2 appear to work only with models matching one of
the architectures in their model zoo, while level 0 is more work to use but can work with just abot
anything, and level 3 is required for models with custom ops or multiple subgraphs.
Check out the <a href="https://docs.xilinx.com/r/en-US/ug1354-xilinx-ai-sdk/Programming-Examples">full docs here</a>.</p>

<p>The API_0 sample is in <code class="language-plaintext highlighter-rouge">Vitis-AI/examples/vai_runtime/resnet50/src/main.cc</code>, so we’ll be
basing our code off of that and some <a href="https://xilinx.github.io/Vitis-AI/3.5/html/doxygen/api/class/classvart_1_1_runner.html">VART documentation</a></p>

<h3 id="code-sample">Code sample</h3>
<p>I’ll be referring to <a href="https://github.com/kyflores/kv260-vitis-flow/blob/main/cpp/main.cc">this file</a>.
Apologies in advance for some janky c++…Maybe if you’re reading this in the future I’ll
have cleaned this up a bit, but some of the VART API stuff threw me for a loop.
my goal here was to get everything that matters into one file, which almost succeeded, except
for a dependency on <a href="https://github.com/nothings/stb">stb_image</a> for reading images. If you don’t
know about <code class="language-plaintext highlighter-rouge">stb</code>, it’s definitely worth a look for single-file, header only code for common tasks.</p>

<p>This all starts off by reading an image (which I separately pulled from the FashionMNIST dataset),
and the xmodel file we produced last time. There’s several subgraphs in the xmodel file, but only
one (for this network) will have the device attribute set to DPU.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">static</span> <span class="k">constexpr</span> <span class="k">const</span> <span class="kt">char</span><span class="o">*</span> <span class="n">MODELNAME</span> <span class="o">=</span> <span class="s">"quantize_result/deploy.xmodel"</span><span class="p">;</span>
<span class="k">auto</span> <span class="n">image</span> <span class="o">=</span> <span class="n">tensor_from_stbimage</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>

<span class="k">auto</span> <span class="n">graph</span> <span class="o">=</span> <span class="n">xir</span><span class="o">::</span><span class="n">Graph</span><span class="o">::</span><span class="n">deserialize</span><span class="p">(</span><span class="n">MODELNAME</span><span class="p">);</span>

<span class="k">auto</span> <span class="n">graph_root</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">get_root_subgraph</span><span class="p">();</span>
<span class="k">auto</span> <span class="n">children</span> <span class="o">=</span> <span class="n">graph_root</span><span class="o">-&gt;</span><span class="n">children_topological_sort</span><span class="p">();</span>

<span class="c1">// Check subgraphs on CLI with `xdputil xmodel quantize_result/deploy.xmodel -l`</span>
<span class="k">const</span> <span class="n">xir</span><span class="o">::</span><span class="n">Subgraph</span><span class="o">*</span> <span class="n">subgraph</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">c</span> <span class="o">:</span> <span class="n">children</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">device</span> <span class="o">=</span> <span class="n">c</span><span class="o">-&gt;</span><span class="n">get_attr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"device"</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Device: "</span> <span class="o">&lt;&lt;</span> <span class="n">device</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">c</span><span class="o">-&gt;</span><span class="n">get_name</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">device</span> <span class="o">==</span> <span class="s">"DPU"</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">subgraph</span> <span class="o">=</span> <span class="n">c</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="k">auto</span> <span class="n">runner</span> <span class="o">=</span> <span class="n">vart</span><span class="o">::</span><span class="n">Runner</span><span class="o">::</span><span class="n">create_runner</span><span class="p">(</span><span class="n">subgraph</span><span class="p">,</span> <span class="s">"run"</span><span class="p">);</span></code></pre></figure>

<p>In this section we get a handle of sorts to the input and output tensors from the graph
This API confused me a bit; I’m used to <code class="language-plaintext highlighter-rouge">tensor</code> storing both the metadata like shape as well
as the data like in Torch, but this VART tensor class seems to be only a description of what
sort of data is required at the inputs and outputs. There’s a separate <code class="language-plaintext highlighter-rouge">TensorBuffer</code> class
in VART that holds the data, and we’ll see that a few lines down.</p>

<p><code class="language-plaintext highlighter-rouge">get_scale</code> is a helper function up above that gets the value of the <code class="language-plaintext highlighter-rouge">fix_point</code> attribute in
the input and output tensors and converts it to a scale factor. This fix_point value tells us
how to convert a floating point value to an int8 in the range this model needs at the inputs and
outputs. Keep in mind for a moment too that at the input, we’re converting float-&gt;int8, and at
the output we’re doing int8-&gt;float, so at the output we need to divide by the scale factor instead.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">auto</span> <span class="n">input_tensors</span> <span class="o">=</span> <span class="n">runner</span><span class="o">-&gt;</span><span class="n">get_input_tensors</span><span class="p">();</span>
<span class="kt">float</span> <span class="n">in_scale</span> <span class="o">=</span> <span class="n">get_scale</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="k">auto</span> <span class="n">output_tensors</span> <span class="o">=</span> <span class="n">runner</span><span class="o">-&gt;</span><span class="n">get_output_tensors</span><span class="p">();</span>
<span class="kt">float</span> <span class="n">out_scale</span> <span class="o">=</span> <span class="n">get_scale</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Input scale "</span> <span class="o">&lt;&lt;</span> <span class="n">in_scale</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Output scale "</span> <span class="o">&lt;&lt;</span> <span class="n">out_scale</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span></code></pre></figure>

<p>The next step is to create TensorBuffers that will hold the actual inputs and outputs.
We’re using this <code class="language-plaintext highlighter-rouge">vart::alloc_cpu_flat_tensor_buffer(input)</code> helper function to do this
for us. <code class="language-plaintext highlighter-rouge">TensorBuffer</code> is actually a base class in VART, and I assume the classes derived from
it hold buffers in different locations, whether that’s main RAM, or an HBM stack private
to the FPGA. Since the Kria’s FPGA doesn’t have its own dedicated memory, we’re sharing the
CPU’s 4GB memory here, thus CpuFlatTensorBuffer. This TensorBuffer has a bit of an awkward
API though, not sure why <code class="language-plaintext highlighter-rouge">CpuFlatTensorBuffer::data()</code> needs to return its data pointer as a
u64 that we need to reinterpret cast back to actually use. Xilinx holds it as a <code class="language-plaintext highlighter-rouge">void*</code> inside
the class and reinterpret_casts it before returning it.</p>

<p>One other note here is that the training and quantiziation code scaled all the image input
values to be between 0 and 1. The inference code needs to do the same so that the calibrated
ranges are valid.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">input</span> <span class="o">:</span> <span class="n">input_tensors</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">t</span> <span class="o">=</span> <span class="n">vart</span><span class="o">::</span><span class="n">alloc_cpu_flat_tensor_buffer</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>

    <span class="c1">// Copy data by ptr into the buffer</span>
    <span class="k">auto</span> <span class="n">tensor_data</span> <span class="o">=</span> <span class="n">t</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
    <span class="c1">// This CpuFlatTensorBuffer class returns a uint64_t by reinterpret casting its 64bit data</span>
    <span class="c1">// pointer into an integer, wtf? Need to undo that here to get something we can use as a ptr</span>
    <span class="kt">int8_t</span><span class="o">*</span> <span class="n">raw_ptr_int</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">int8_t</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor_data</span><span class="p">));</span>
    <span class="kt">size_t</span> <span class="n">raw_bytes</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor_data</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Input tensor is "</span> <span class="o">&lt;&lt;</span> <span class="n">raw_bytes</span> <span class="o">&lt;&lt;</span> <span class="s">" bytes long"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">raw_bytes</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// In our training code, the uint8 images are squished to the range (0, 1)</span>
        <span class="c1">// before being input to the network, so replicate that here.</span>
        <span class="kt">float</span> <span class="n">tmp</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">/</span> <span class="mi">256</span><span class="p">;</span>

        <span class="n">raw_ptr_int</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int8_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tmp</span> <span class="o">*</span> <span class="n">in_scale</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">input_tensor_buffers</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">t</span><span class="p">));</span>
<span class="p">}</span></code></pre></figure>

<p>Before launching the DPU, we need to call <code class="language-plaintext highlighter-rouge">sync_for_write</code> on our input, then gather
the TensorBuffer pointers together for <code class="language-plaintext highlighter-rouge">execute_async</code>. This looks kind of hacky to me,
so I might be using the API wrong here…but up above <code class="language-plaintext highlighter-rouge">alloc_cpu_flat_tensor_buffer</code> returns
its buffers behind a unique_ptr, but execute_async requires a raw pointer.</p>

<p>Then we wait for the computation to finish and finally sync the output buffer.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="n">input</span> <span class="o">:</span> <span class="n">input_tensor_buffers</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">input</span><span class="o">-&gt;</span><span class="n">sync_for_write</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">-&gt;</span><span class="n">get_tensor</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">get_data_size</span><span class="p">()</span> <span class="o">/</span>
            <span class="n">input</span><span class="o">-&gt;</span><span class="n">get_tensor</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">]);</span>
<span class="p">}</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Executing runner..."</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">vart</span><span class="o">::</span><span class="n">TensorBuffer</span><span class="o">*&gt;</span> <span class="n">input_ptrs</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="n">ptr</span> <span class="o">:</span> <span class="n">input_tensor_buffers</span><span class="p">)</span> <span class="p">{</span> <span class="n">input_ptrs</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">ptr</span><span class="p">.</span><span class="n">get</span><span class="p">());</span> <span class="p">}</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">vart</span><span class="o">::</span><span class="n">TensorBuffer</span><span class="o">*&gt;</span> <span class="n">output_ptrs</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="n">ptr</span> <span class="o">:</span> <span class="n">output_tensor_buffers</span><span class="p">)</span> <span class="p">{</span> <span class="n">output_ptrs</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">ptr</span><span class="p">.</span><span class="n">get</span><span class="p">());</span> <span class="p">}</span>

<span class="k">auto</span> <span class="n">v</span> <span class="o">=</span> <span class="n">runner</span><span class="o">-&gt;</span><span class="n">execute_async</span><span class="p">(</span><span class="n">input_ptrs</span><span class="p">,</span> <span class="n">output_ptrs</span><span class="p">);</span>
<span class="k">auto</span> <span class="n">status</span> <span class="o">=</span> <span class="n">runner</span><span class="o">-&gt;</span><span class="n">wait</span><span class="p">((</span><span class="kt">int</span><span class="p">)</span><span class="n">v</span><span class="p">.</span><span class="n">first</span><span class="p">,</span> <span class="mi">1000000000</span><span class="p">);</span>

<span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="n">output</span> <span class="o">:</span> <span class="n">output_tensor_buffers</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">output</span><span class="o">-&gt;</span><span class="n">sync_for_read</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">output</span><span class="o">-&gt;</span><span class="n">get_tensor</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">get_data_size</span><span class="p">()</span> <span class="o">/</span>
    <span class="n">output</span><span class="o">-&gt;</span><span class="n">get_tensor</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">]);</span>
<span class="p">}</span></code></pre></figure>

<p>We’re basically done at this point. To wrap up we just run softmax over the outputs.
Unfortunately there’s no built softmax operator on the DPU, and I didn’t see one in
VART either, so here’s a naive softmax implementation to use instead. Note that this
won’t work very well if your inputs get too large…</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="nf">print_softmax</span><span class="p">(</span><span class="kt">int8_t</span><span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">len</span><span class="p">,</span> <span class="kt">float</span> <span class="n">scale</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">double</span> <span class="n">denom</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">vec</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">len</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">tmp</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">scale</span><span class="p">);</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">exp</span><span class="p">(</span><span class="n">tmp</span><span class="p">);</span>
        <span class="n">denom</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">;</span>
        <span class="n">vec</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">tmp</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">len</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">vec</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">/</span> <span class="n">denom</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="o">&amp;</span> <span class="n">out_buf</span> <span class="o">=</span> <span class="n">output_tensor_buffers</span><span class="p">.</span><span class="n">back</span><span class="p">();</span>
<span class="k">auto</span> <span class="n">out_data</span> <span class="o">=</span> <span class="n">out_buf</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
<span class="kt">int8_t</span><span class="o">*</span> <span class="n">raw_ptr_int</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">int8_t</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">out_data</span><span class="p">));</span>
<span class="kt">size_t</span> <span class="n">raw_bytes</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">out_data</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Output tensor is "</span> <span class="o">&lt;&lt;</span> <span class="n">raw_bytes</span> <span class="o">&lt;&lt;</span> <span class="s">" bytes long"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">print_softmax</span><span class="p">(</span><span class="n">raw_ptr_int</span><span class="p">,</span> <span class="n">raw_bytes</span><span class="p">,</span> <span class="n">out_scale</span><span class="p">);</span></code></pre></figure>

<p>At long last…our results! This image was from label index 1 (from 0 to 9),
and we can see at the bottom that we predicted 99.63% confidence for label 1.</p>

<p><img src="/assets/fmnist_1.jpg" alt="sample" width="250" /></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x, y, n: 28 28 1
Device: USER
subgraph_MyCnn__input_0
Device: DPU
subgraph_MyCnn__MyCnn_Conv2d_conv1_1__ret_3
Device: CPU
subgraph_MyCnn__MyCnn_Linear_lin2__ret_fix_
Input scale 64
Output scale 8
Populating input tensor
1 Input(s)
Input tensor is 784 bytes long
1 Output(s)
Executing runner...
Output tensor is 10 bytes long

3.52274e-05 0.996319 2.55186e-06 0.00359329 4.52328e-05 9.20345e-09 3.71294e-06 2.20779e-08 6.45211e-07 1.63135e-07
</code></pre></div></div>

  </div><a class="u-url" href="/xilinx/edge_ai/inference/vitis/2023/10/01/inference.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Kyle&#39;s code attic</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Kyle&#39;s code attic</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/kyflores"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">kyflores</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my page. I plan to use this site to document some of my projects around embedded systems, edge AI, computer vision, and a mix of other things.
Frequently when scouring the internet for solutions to obscure problems, I find the real answer on someone&#39;s personal dev blog. I hope others can find the same value here.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
